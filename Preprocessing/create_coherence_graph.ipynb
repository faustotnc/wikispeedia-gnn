{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import umap\n",
    "from umap.umap_ import UMAP\n",
    "import hdbscan\n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import unquote\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages = pd.read_csv(\"../data/Wikispeedia/articles.tsv\", sep=\"\\t\", skiprows=12, names=[\"name\"])\n",
    "\n",
    "# processed_docs = []\n",
    "# for doc in tqdm(pages[\"name\"]):\n",
    "#     with open(f\"../data/Wikispeedia/plaintext_articles/{doc}.txt\", \"r\") as fp:\n",
    "#         processed_docs.append({\n",
    "#             \"title\": unquote(doc),\n",
    "#             \"plain_text\": \" \".join(fp.read().replace(\"#copyright\\n\\n\", '').split(\" \")[:4000]).strip()\n",
    "#         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = pd.DataFrame(processed_docs)\n",
    "# dataset.to_csv(\"../data/full_text_data.csv\", index=False)\n",
    "\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>plain_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Áedán_mac_Gabráin</td>\n",
       "      <td>Áedán mac Gabráin\\n\\n2007 Schools Wikipedia Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Åland</td>\n",
       "      <td>Åland\\n\\n2007 Schools Wikipedia Selection. Rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Édouard_Manet</td>\n",
       "      <td>Édouard Manet\\n\\n2007 Schools Wikipedia Select...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Éire</td>\n",
       "      <td>Éire\\n\\n2007 Schools Wikipedia Selection. Rela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Óengus_I_of_the_Picts</td>\n",
       "      <td>Óengus I of the Picts\\n\\n2007 Schools Wikipedi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>Zionism</td>\n",
       "      <td>Zionism\\n\\n2007 Schools Wikipedia Selection. R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>Zirconium</td>\n",
       "      <td>Zirconium\\n\\n2007 Schools Wikipedia Selection....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4601</th>\n",
       "      <td>Zoroaster</td>\n",
       "      <td>Zoroaster\\n\\n2007 Schools Wikipedia Selection....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4602</th>\n",
       "      <td>Zuid-Gelders</td>\n",
       "      <td>Zuid-Gelders\\n\\n2007 Schools Wikipedia Selecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4603</th>\n",
       "      <td>Zulu</td>\n",
       "      <td>Zulu\\n\\n2007 Schools Wikipedia Selection. Rela...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4604 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      title                                         plain_text\n",
       "0         Áedán_mac_Gabráin  Áedán mac Gabráin\\n\\n2007 Schools Wikipedia Se...\n",
       "1                     Åland  Åland\\n\\n2007 Schools Wikipedia Selection. Rel...\n",
       "2             Édouard_Manet  Édouard Manet\\n\\n2007 Schools Wikipedia Select...\n",
       "3                      Éire  Éire\\n\\n2007 Schools Wikipedia Selection. Rela...\n",
       "4     Óengus_I_of_the_Picts  Óengus I of the Picts\\n\\n2007 Schools Wikipedi...\n",
       "...                     ...                                                ...\n",
       "4599                Zionism  Zionism\\n\\n2007 Schools Wikipedia Selection. R...\n",
       "4600              Zirconium  Zirconium\\n\\n2007 Schools Wikipedia Selection....\n",
       "4601              Zoroaster  Zoroaster\\n\\n2007 Schools Wikipedia Selection....\n",
       "4602           Zuid-Gelders  Zuid-Gelders\\n\\n2007 Schools Wikipedia Selecti...\n",
       "4603                   Zulu  Zulu\\n\\n2007 Schools Wikipedia Selection. Rela...\n",
       "\n",
       "[4604 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"../data/full_text_data.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '../data/gpt4_embeddings.pkl' loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "def get_OpenAI_embeddings(texts, model=\"text-embedding-3-small\"):\n",
    "    texts = [text.replace(\"\\n\", \" \") for text in texts]  # Clean up text\n",
    "    response = client.embeddings.create(input=texts, model=model)\n",
    "    return [res_data.embedding for res_data in response.data]\n",
    "\n",
    "\n",
    "def batched_embeddings(texts, batch_size=16, model=\"text-embedding-3-small\"):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        batch_embeddings = get_OpenAI_embeddings(batch_texts, model=model)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "        time.sleep(0.75)  # Prevent making too many requests too fast\n",
    "    return torch.tensor(embeddings)\n",
    "\n",
    "\n",
    "def extract_embeddings(text, file):\n",
    "    try:\n",
    "        with open(f\"{file}\", 'rb') as handle:\n",
    "            obj = pickle.load(handle)\n",
    "            embeddings = obj[\"embeddings\"]\n",
    "            del obj\n",
    "\n",
    "            print(f\"File '{file}' loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Could not find file '{file}'. Regenerating the embeddings.\")\n",
    "        embeddings = batched_embeddings(text, batch_size=16)\n",
    "\n",
    "        with open(f\"{file}\", 'wb') as handle:\n",
    "            pickle.dump(\n",
    "                obj={\"embeddings\": embeddings},\n",
    "                file=handle,\n",
    "                protocol=pickle.HIGHEST_PROTOCOL\n",
    "            )\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "embeddings = extract_embeddings(\n",
    "    text=dataset[\"plain_text\"].tolist(),\n",
    "    file=\"../data/gpt4_embeddings.pkl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = UMAP(\n",
    "    n_neighbors=32,\n",
    "    n_components=48,\n",
    "    min_dist=0,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42,\n",
    "    n_jobs=1,\n",
    "    low_memory=True\n",
    ")\n",
    "\n",
    "low_dim_mapper = umap_model.fit(embeddings)\n",
    "low_dim_embeds = low_dim_mapper.embedding_\n",
    "\n",
    "# Center the embeddings around the mean\n",
    "low_dim_embeds = low_dim_embeds - np.mean(low_dim_embeds, axis=0)\n",
    "\n",
    "\n",
    "hdbscan_model = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=8,\n",
    "    cluster_selection_method=\"eom\",\n",
    "    prediction_data=True,\n",
    ").fit(low_dim_embeds)\n",
    "\n",
    "cluster_label_probs = hdbscan.prediction.all_points_membership_vectors(\n",
    "    hdbscan_model\n",
    ")\n",
    "\n",
    "cluster_labels = cluster_label_probs.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_base_coherence_matrix(embeds, cluster_probs: np.ndarray):\n",
    "    # Compute cosine similarity and fix rounding errors\n",
    "    # Here we use the dot product, since the high-dimensional embeddings are normalized\n",
    "    cos_sim = np.clip(embeds @ embeds.T, -1, 1)\n",
    "\n",
    "    # Compute angular similarity\n",
    "    ang_sim = 1 - np.arccos(cos_sim) / np.pi\n",
    "\n",
    "    # Diagonals may sometimes be NaN. Probably from rounding errors\n",
    "    # We set them to 0 here since we're not interested in self-loops.\n",
    "    np.fill_diagonal(ang_sim, 0)\n",
    "\n",
    "    # Compute topic similarity\n",
    "    topic_sim = 1 - distance.cdist(\n",
    "        XA=cluster_probs,\n",
    "        XB=cluster_probs,\n",
    "        metric='jensenshannon'\n",
    "    )\n",
    "\n",
    "    coherence_matrix = (ang_sim * topic_sim) ** (1/2)\n",
    "\n",
    "    # Warn user about NaN values.\n",
    "    if np.isnan(cos_sim).any():\n",
    "        print(\"WARNING: Cosine Similarity matrix contains NaN values.\")\n",
    "    if np.isnan(ang_sim).any():\n",
    "        print(\"WARNING: Angular Similarity matrix contains NaN values.\")\n",
    "    if np.isnan(topic_sim).any():\n",
    "        print(\"WARNING: Topic Similarity matrix contains NaN values.\")\n",
    "    if np.isnan(coherence_matrix).any():\n",
    "        print(\"WARNING: Coherence matrix contains NaN values.\")\n",
    "\n",
    "    return coherence_matrix, ang_sim, topic_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_matrix, ang_sim, topic_sim = compute_base_coherence_matrix(\n",
    "    embeddings.numpy(),\n",
    "    cluster_label_probs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.33182796, 0.31785333, ..., 0.33567734, 0.3096299 ,\n",
       "        0.31812011],\n",
       "       [0.33182796, 0.        , 0.31861573, ..., 0.59975807, 0.34623951,\n",
       "        0.33235212],\n",
       "       [0.31785333, 0.31861573, 0.        , ..., 0.32629773, 0.30271915,\n",
       "        0.30777674],\n",
       "       ...,\n",
       "       [0.33567734, 0.59975807, 0.32629773, ..., 0.        , 0.3185637 ,\n",
       "        0.34137462],\n",
       "       [0.3096299 , 0.34623951, 0.30271915, ..., 0.3185637 , 0.        ,\n",
       "        0.31672138],\n",
       "       [0.31812011, 0.33235212, 0.30777674, ..., 0.34137462, 0.31672138,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../data/coherence_graph.pkl\", 'wb') as handle:\n",
    "    pickle.dump(\n",
    "        obj=coherence_matrix,\n",
    "        file=handle,\n",
    "        protocol=pickle.HIGHEST_PROTOCOL\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
