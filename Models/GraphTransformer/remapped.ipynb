{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from urllib.parse import unquote\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, GraphNorm\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from torch_geometric.nn import TransformerConv, global_mean_pool\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text data\n",
    "data = pd.read_csv(f\"../../data/full_text_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load links\n",
    "links = pd.read_csv(\"../../data/Wikispeedia/links.tsv\", sep=\"\\t\", names=[\"src\", \"tgt\"], skiprows=12)\n",
    "links[\"src\"] = links[\"src\"].map(lambda x: unquote(x))\n",
    "links[\"tgt\"] = links[\"tgt\"].map(lambda x: unquote(x))\n",
    "\n",
    "# Create adjacency matrix\n",
    "ordered_data_titles = data[\"title\"].tolist()\n",
    "src_indices = links[\"src\"].map(lambda x: ordered_data_titles.index(x))\n",
    "tgt_indices = links[\"tgt\"].map(lambda x: ordered_data_titles.index(x))\n",
    "A = torch.zeros((len(ordered_data_titles), len(ordered_data_titles)))\n",
    "A[src_indices, tgt_indices] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kt/8729r9s16_15cc905qks8zl40000gn/T/ipykernel_37340/3991216429.py:6: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  edge_features = A * coherence_graph\n"
     ]
    }
   ],
   "source": [
    "# Load coherence graph\n",
    "with open(\"../../data/coherence_graph.pkl\", 'rb') as handle:\n",
    "    coherence_graph = pickle.load(handle)\n",
    "\n",
    "# Combine coherence graph with base links\n",
    "edge_features = A * coherence_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kt/8729r9s16_15cc905qks8zl40000gn/T/ipykernel_37340/3588909332.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  node_static_embeddings = torch.tensor(node_static_embeddings, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "# Load node embeddings\n",
    "with open(\"../../data/gpt4_embeddings.pkl\", 'rb') as handle:\n",
    "    obj = pickle.load(handle)\n",
    "    node_static_embeddings = obj[\"embeddings\"]\n",
    "    del obj\n",
    "node_static_embeddings = torch.tensor(node_static_embeddings, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load user-extracted paths\n",
    "paths_data = pd.read_csv(f\"../../data/paths_no_back_links.tsv\", sep=\"\\t\")\n",
    "paths_data = paths_data[~(paths_data[\"rating\"].isna())]\n",
    "\n",
    "# Filter paths with at least four distinct pages\n",
    "# paths_data = paths_data[paths_data[\"path\"].apply(lambda x: len(set(x.split(\";\"))) >= 4)]\n",
    "\n",
    "# Map titles to indices\n",
    "title_to_index = {unquote(title): idx for idx, title in enumerate(data['title'])}\n",
    "paths = paths_data['path'].apply(\n",
    "    lambda path: [title_to_index[unquote(title)] for title in path.split(';')]\n",
    ").tolist()\n",
    "\n",
    "# Convert ratings to binary labels\n",
    "ratings = (paths_data['rating'] - 1).tolist()  # 0-indexed ratings from 0 to 4\n",
    "\n",
    "# # Map ratings [0, 1] to 0, and ratings [2, 3, 4] to 1\n",
    "ratings = [0 if r <= 1 else 1 for r in ratings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiahaoxu/Github/wikispeedia-gnn/.venv/lib/python3.9/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "class PathDataset(Dataset):\n",
    "    def __init__(self, paths, ratings, node_embeddings, edge_features):\n",
    "        self.paths = paths\n",
    "        self.ratings = ratings\n",
    "        self.node_embeddings = node_embeddings\n",
    "        self.edge_features = edge_features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        rating = self.ratings[idx]\n",
    "        nodes, edge_index, edge_weight = self.get_subgraph_edges(path)\n",
    "\n",
    "        x = self.node_embeddings[nodes]\n",
    "\n",
    "        data = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_weight=edge_weight,\n",
    "            y=torch.tensor([rating], dtype=torch.long)\n",
    "        )\n",
    "        return data\n",
    "\n",
    "    def get_subgraph_edges(self, path):\n",
    "        nodes = list(set(path))\n",
    "        node_to_idx = {node: idx for idx, node in enumerate(nodes)}\n",
    "        edges = []\n",
    "        edge_weights = []\n",
    "        for i in nodes:\n",
    "            for j in nodes:\n",
    "                weight = self.edge_features[i, j]\n",
    "                if weight > 0:\n",
    "                    edges.append([node_to_idx[i], node_to_idx[j]])\n",
    "                    edge_weights.append(weight)\n",
    "        if edges:\n",
    "            edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "            edge_weight = torch.tensor(edge_weights, dtype=torch.float)\n",
    "        else:\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "            edge_weight = torch.tensor([], dtype=torch.float)\n",
    "        return nodes, edge_index, edge_weight\n",
    "\n",
    "# Create dataset\n",
    "dataset = PathDataset(paths, ratings, node_static_embeddings, edge_features)\n",
    "\n",
    "# Split dataset\n",
    "train_ratio = 0.85\n",
    "val_ratio = 0.05\n",
    "test_ratio = 0.1\n",
    "total_size = len(dataset)\n",
    "train_size = int(train_ratio * total_size)\n",
    "val_size = int(val_ratio * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 6\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7851, 1.2149])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counts occurrences of each class\n",
    "class_counts = torch.bincount(\n",
    "    torch.tensor(ratings)[train_dataset.indices].to(torch.int64)\n",
    ")\n",
    "\n",
    "# Calculate weights as the inverse of class frequencies\n",
    "class_weights = 1.0 / class_counts.float()\n",
    "\n",
    "# Normalize the weights so that they sum to the number of classes\n",
    "class_weights = class_weights / class_weights.sum() * len(class_counts)\n",
    "\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import TransformerConv, global_mean_pool, GraphNorm\n",
    "\n",
    "class GraphTransformerModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes, heads=4, dropout=0.2):\n",
    "        super(GraphTransformerModel, self).__init__()\n",
    "        self.conv1 = TransformerConv(\n",
    "            in_channels, hidden_channels // heads, heads=heads, edge_dim=1, dropout=dropout)\n",
    "        self.norm1 = GraphNorm(hidden_channels)\n",
    "        \n",
    "        self.conv2 = TransformerConv(\n",
    "            hidden_channels, hidden_channels // heads, heads=heads, edge_dim=1, dropout=dropout)\n",
    "        self.norm2 = GraphNorm(hidden_channels)\n",
    "        \n",
    "        # Additional MLP Layer\n",
    "        self.mlp = nn.Linear(hidden_channels, hidden_channels)\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_channels, num_classes)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_weight\n",
    "\n",
    "        # Reshape edge_weight to [num_edges, 1] if it exists\n",
    "        edge_attr = edge_weight.view(-1, 1) if edge_weight is not None else None\n",
    "\n",
    "        # First Transformer Conv Layer\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = self.norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Second Transformer Conv Layer\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = self.norm2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Global Mean Pooling\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "\n",
    "        # MLP Layer\n",
    "        x = self.mlp(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Final Classifier\n",
    "        x = self.classifier(x)\n",
    "        return x  # Return raw logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "model = GraphTransformerModel(\n",
    "    in_channels=node_static_embeddings.shape[1],\n",
    "    hidden_channels=64,\n",
    "    num_classes=2,\n",
    "    heads=4,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in tqdm(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == data.y).sum().item()\n",
    "            total += data.num_graphs\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4038/4038 [00:19<00:00, 206.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6678, Val Acc: 0.6407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4038/4038 [00:19<00:00, 206.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Loss: 0.6569, Val Acc: 0.5565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4038/4038 [00:19<00:00, 205.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Loss: 0.6495, Val Acc: 0.7039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4038/4038 [00:19<00:00, 205.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Loss: 0.6493, Val Acc: 0.6470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4038/4038 [00:19<00:00, 205.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Loss: 0.6419, Val Acc: 0.6505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4038/4038 [00:19<00:00, 205.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Loss: 0.6444, Val Acc: 0.6435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4038/4038 [00:19<00:00, 204.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Loss: 0.6439, Val Acc: 0.5705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4038/4038 [00:19<00:00, 203.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Loss: 0.6371, Val Acc: 0.6526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4038/4038 [00:19<00:00, 204.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Loss: 0.6418, Val Acc: 0.5495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4038/4038 [00:19<00:00, 203.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 0.6352, Val Acc: 0.6533\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0\n",
    "for epoch in range(1, 11):\n",
    "    loss = train()\n",
    "    val_preds, val_labels = evaluate(val_loader)\n",
    "    val_acc = (np.array(val_preds) == np.array(val_labels)).mean()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), '.best_rmapped_model.pth')\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kt/8729r9s16_15cc905qks8zl40000gn/T/ipykernel_37340/1131010271.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('.best_rmapped_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7043142756927394\n",
      "Test Accuracy: 0.7043\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load('.best_rmapped_model.pth'))\n",
    "\n",
    "test_acc = evaluate(test_loader)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def conf_matrix(model, dataset_loader):\n",
    "    # Pass the testing loader through the model\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataset_loader:\n",
    "            batch = batch.to(device)  # Move batch to the same device as the model\n",
    "            logits = model(batch)  # Pass the entire batch to the model\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            labels = batch.y.cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your trained model is named 'model' and your DataLoader is 'test_loader'\n",
    "conf_matrix(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate        = 0.01\n",
      "weight_decay         = 0.0005\n",
      "num_epochs           = 10\n",
      "batch_size           = 6\n",
      "------\n",
      "<bound method Module.parameters of GraphTransformerModel(\n",
      "  (conv1): TransformerConv(1536, 16, heads=4)\n",
      "  (norm1): GraphNorm(64)\n",
      "  (conv2): TransformerConv(64, 16, heads=4)\n",
      "  (norm2): GraphNorm(64)\n",
      "  (mlp): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (classifier): Linear(in_features=64, out_features=2, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# Function to print the hyperparameters\n",
    "def print_hyperparameters():\n",
    "    print(\"learning_rate        =\", 1e-2)\n",
    "    print(\"weight_decay         =\", 5e-4)\n",
    "    print(\"num_epochs           =\", 10)\n",
    "    print(\"batch_size           =\", 6)\n",
    "    print(\"------\")\n",
    "    # print(\"num_gt_layers        =\", 2)\n",
    "    # print(\"    -\")\n",
    "    # print(\"    input_channels   =\", node_static_embeddings.shape[1])\n",
    "    # print(\"    hidden_channels  =\", 16)  # Updated value if changed\n",
    "    # print(\"    -\")\n",
    "    # print(\"    input_channels   =\", 64)\n",
    "    # print(\"    hidden_channels  =\", 16)  # Updated value if changed\n",
    "    # print(\"num_linear_mlp       =\", 1)\n",
    "    # print(\"    layer_shape      =\", (64,64))  # Updated value if changed\n",
    "\n",
    "# Call the function to print the hyperparameters\n",
    "print_hyperparameters()\n",
    "print(model.parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
