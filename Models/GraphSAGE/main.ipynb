{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faustogerman/miniforge3/envs/geometric/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_scatter\n",
    "import pickle\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "\n",
    "from urllib.parse import unquote\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from dataclasses import dataclass\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>plain_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Áedán_mac_Gabráin</td>\n",
       "      <td>Áedán mac Gabráin\\n\\n2007 Schools Wikipedia Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Åland</td>\n",
       "      <td>Åland\\n\\n2007 Schools Wikipedia Selection. Rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Édouard_Manet</td>\n",
       "      <td>Édouard Manet\\n\\n2007 Schools Wikipedia Select...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Éire</td>\n",
       "      <td>Éire\\n\\n2007 Schools Wikipedia Selection. Rela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Óengus_I_of_the_Picts</td>\n",
       "      <td>Óengus I of the Picts\\n\\n2007 Schools Wikipedi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>Zionism</td>\n",
       "      <td>Zionism\\n\\n2007 Schools Wikipedia Selection. R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>Zirconium</td>\n",
       "      <td>Zirconium\\n\\n2007 Schools Wikipedia Selection....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4601</th>\n",
       "      <td>Zoroaster</td>\n",
       "      <td>Zoroaster\\n\\n2007 Schools Wikipedia Selection....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4602</th>\n",
       "      <td>Zuid-Gelders</td>\n",
       "      <td>Zuid-Gelders\\n\\n2007 Schools Wikipedia Selecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4603</th>\n",
       "      <td>Zulu</td>\n",
       "      <td>Zulu\\n\\n2007 Schools Wikipedia Selection. Rela...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4604 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      title                                         plain_text\n",
       "0         Áedán_mac_Gabráin  Áedán mac Gabráin\\n\\n2007 Schools Wikipedia Se...\n",
       "1                     Åland  Åland\\n\\n2007 Schools Wikipedia Selection. Rel...\n",
       "2             Édouard_Manet  Édouard Manet\\n\\n2007 Schools Wikipedia Select...\n",
       "3                      Éire  Éire\\n\\n2007 Schools Wikipedia Selection. Rela...\n",
       "4     Óengus_I_of_the_Picts  Óengus I of the Picts\\n\\n2007 Schools Wikipedi...\n",
       "...                     ...                                                ...\n",
       "4599                Zionism  Zionism\\n\\n2007 Schools Wikipedia Selection. R...\n",
       "4600              Zirconium  Zirconium\\n\\n2007 Schools Wikipedia Selection....\n",
       "4601              Zoroaster  Zoroaster\\n\\n2007 Schools Wikipedia Selection....\n",
       "4602           Zuid-Gelders  Zuid-Gelders\\n\\n2007 Schools Wikipedia Selecti...\n",
       "4603                   Zulu  Zulu\\n\\n2007 Schools Wikipedia Selection. Rela...\n",
       "\n",
       "[4604 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(f\"../../data/full_text_data.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>%C3%81ed%C3%A1n_mac_Gabr%C3%A1in</td>\n",
       "      <td>Bede</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>%C3%81ed%C3%A1n_mac_Gabr%C3%A1in</td>\n",
       "      <td>Columba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>%C3%81ed%C3%A1n_mac_Gabr%C3%A1in</td>\n",
       "      <td>D%C3%A1l_Riata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>%C3%81ed%C3%A1n_mac_Gabr%C3%A1in</td>\n",
       "      <td>Great_Britain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>%C3%81ed%C3%A1n_mac_Gabr%C3%A1in</td>\n",
       "      <td>Ireland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119877</th>\n",
       "      <td>Zulu</td>\n",
       "      <td>South_Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119878</th>\n",
       "      <td>Zulu</td>\n",
       "      <td>Swaziland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119879</th>\n",
       "      <td>Zulu</td>\n",
       "      <td>United_Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119880</th>\n",
       "      <td>Zulu</td>\n",
       "      <td>Zambia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119881</th>\n",
       "      <td>Zulu</td>\n",
       "      <td>Zimbabwe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119882 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     src             tgt\n",
       "0       %C3%81ed%C3%A1n_mac_Gabr%C3%A1in            Bede\n",
       "1       %C3%81ed%C3%A1n_mac_Gabr%C3%A1in         Columba\n",
       "2       %C3%81ed%C3%A1n_mac_Gabr%C3%A1in  D%C3%A1l_Riata\n",
       "3       %C3%81ed%C3%A1n_mac_Gabr%C3%A1in   Great_Britain\n",
       "4       %C3%81ed%C3%A1n_mac_Gabr%C3%A1in         Ireland\n",
       "...                                  ...             ...\n",
       "119877                              Zulu    South_Africa\n",
       "119878                              Zulu       Swaziland\n",
       "119879                              Zulu  United_Kingdom\n",
       "119880                              Zulu          Zambia\n",
       "119881                              Zulu        Zimbabwe\n",
       "\n",
       "[119882 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = pd.read_csv(\"../../data/Wikispeedia/links.tsv\",\n",
    "                    sep=\"\\t\", names=[\"src\", \"tgt\"], skiprows=12)\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an adjacency matrix from the links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links[\"src\"] = links[\"src\"].map(lambda x: unquote(x))\n",
    "links[\"tgt\"] = links[\"tgt\"].map(lambda x: unquote(x))\n",
    "\n",
    "ordered_data_titles = data[\"title\"].tolist()\n",
    "\n",
    "src_indices = links[\"src\"].map(lambda x: ordered_data_titles.index(x))\n",
    "tgt_indices = links[\"tgt\"].map(lambda x: ordered_data_titles.index(x))\n",
    "\n",
    "A = torch.zeros((len(ordered_data_titles), len(ordered_data_titles)))\n",
    "A[src_indices, tgt_indices] = 1\n",
    "\n",
    "A  # This is the base structure of the Wikipedia network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the coherence graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence graph loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.33138034, 0.31785333, ..., 0.33416559, 0.3096299 ,\n",
       "        0.3338204 ],\n",
       "       [0.33138034, 0.        , 0.31811661, ..., 0.59573573, 0.34740373,\n",
       "        0.66406903],\n",
       "       [0.31785333, 0.31811661, 0.        , ..., 0.32476778, 0.30271915,\n",
       "        0.32060009],\n",
       "       ...,\n",
       "       [0.33416559, 0.59573573, 0.32476778, ..., 0.        , 0.31785316,\n",
       "        0.67388399],\n",
       "       [0.3096299 , 0.34740373, 0.30271915, ..., 0.31785316, 0.        ,\n",
       "        0.33336595],\n",
       "       [0.3338204 , 0.66406903, 0.32060009, ..., 0.67388399, 0.33336595,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../../data/coherence_graph.pkl\", 'rb') as handle:\n",
    "    coherence_graph = pickle.load(handle)\n",
    "    print(f\"Coherence graph loaded successfully.\")\n",
    "\n",
    "coherence_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the Coherence Graph and Base Links to form Semantic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_features = A * coherence_graph\n",
    "edge_features  # This is the base structure of the Wikipedia network, with coherence scores as edge features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Node Features (OpenAI Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '../../data/gpt4_embeddings.pkl' loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../data/gpt4_embeddings.pkl\", 'rb') as handle:\n",
    "    obj = pickle.load(handle)\n",
    "    node_static_embeddings = obj[\"embeddings\"]\n",
    "    del obj\n",
    "    print(f\"File '../../data/gpt4_embeddings.pkl' loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the User-Extracted Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashedIpAddress</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>durationInSec</th>\n",
       "      <th>path</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3824310e536af032</td>\n",
       "      <td>1344753412</td>\n",
       "      <td>88</td>\n",
       "      <td>14th_century;Europe;Africa;Atlantic_slave_trad...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>015245d773376aab</td>\n",
       "      <td>1366730828</td>\n",
       "      <td>175</td>\n",
       "      <td>14th_century;Italy;Roman_Catholic_Church;HIV;R...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>36dabfa133b20e3c</td>\n",
       "      <td>1249525912</td>\n",
       "      <td>112</td>\n",
       "      <td>14th_century;China;Gunpowder;Fire</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20418ff4797f96be</td>\n",
       "      <td>1229188046</td>\n",
       "      <td>139</td>\n",
       "      <td>14th_century;Time;Isaac_Newton;Light;Color;Rai...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>08888b1b428dd90e</td>\n",
       "      <td>1232241510</td>\n",
       "      <td>74</td>\n",
       "      <td>14th_century;Time;Light;Rainbow</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51310</th>\n",
       "      <td>55150dce4acb74c8</td>\n",
       "      <td>1340891297</td>\n",
       "      <td>117</td>\n",
       "      <td>Women%27s_rights;Property;Clothing;Weather;Autumn</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51311</th>\n",
       "      <td>4753cde919cd5ce5</td>\n",
       "      <td>1348670636</td>\n",
       "      <td>418</td>\n",
       "      <td>Work_%28thermodynamics%29;Energy;Aristotle;Poe...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51312</th>\n",
       "      <td>052ba30a41ff5a05</td>\n",
       "      <td>1371787558</td>\n",
       "      <td>133</td>\n",
       "      <td>Work_%28thermodynamics%29;Cambridge;City_statu...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51314</th>\n",
       "      <td>2ef7ac844cefda58</td>\n",
       "      <td>1300254138</td>\n",
       "      <td>165</td>\n",
       "      <td>Yagan;Folklore;Brothers_Grimm;Folklore;19th_ce...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51316</th>\n",
       "      <td>19f8284371753362</td>\n",
       "      <td>1298792567</td>\n",
       "      <td>56</td>\n",
       "      <td>Yarralumla%2C_Australian_Capital_Territory;Aus...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28501 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        hashedIpAddress   timestamp  durationInSec  \\\n",
       "1      3824310e536af032  1344753412             88   \n",
       "4      015245d773376aab  1366730828            175   \n",
       "6      36dabfa133b20e3c  1249525912            112   \n",
       "7      20418ff4797f96be  1229188046            139   \n",
       "8      08888b1b428dd90e  1232241510             74   \n",
       "...                 ...         ...            ...   \n",
       "51310  55150dce4acb74c8  1340891297            117   \n",
       "51311  4753cde919cd5ce5  1348670636            418   \n",
       "51312  052ba30a41ff5a05  1371787558            133   \n",
       "51314  2ef7ac844cefda58  1300254138            165   \n",
       "51316  19f8284371753362  1298792567             56   \n",
       "\n",
       "                                                    path  rating  \n",
       "1      14th_century;Europe;Africa;Atlantic_slave_trad...     3.0  \n",
       "4      14th_century;Italy;Roman_Catholic_Church;HIV;R...     3.0  \n",
       "6                      14th_century;China;Gunpowder;Fire     2.0  \n",
       "7      14th_century;Time;Isaac_Newton;Light;Color;Rai...     1.0  \n",
       "8                        14th_century;Time;Light;Rainbow     3.0  \n",
       "...                                                  ...     ...  \n",
       "51310  Women%27s_rights;Property;Clothing;Weather;Autumn     5.0  \n",
       "51311  Work_%28thermodynamics%29;Energy;Aristotle;Poe...     3.0  \n",
       "51312  Work_%28thermodynamics%29;Cambridge;City_statu...     3.0  \n",
       "51314  Yagan;Folklore;Brothers_Grimm;Folklore;19th_ce...     3.0  \n",
       "51316  Yarralumla%2C_Australian_Capital_Territory;Aus...     1.0  \n",
       "\n",
       "[28501 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths_data = pd.read_csv(f\"../../data/paths_no_back_links.tsv\", sep=\"\\t\")\n",
    "paths_data = paths_data[~(paths_data[\"rating\"].isna())]\n",
    "\n",
    "paths_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map the Path Titles to Node Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from title to index\n",
    "title_to_index = {unquote(title): idx for idx, title in enumerate(data['title'])}\n",
    "\n",
    "# Convert each path to a list of indices\n",
    "paths = paths_data['path'].apply(\n",
    "    lambda path: [title_to_index[unquote(title)] for title in path.split(';')]\n",
    ").tolist()\n",
    "\n",
    "# Ratings start at 1. We make them 0-indexed here to be compatible with CrossEntropyLoss\n",
    "ratings = (paths_data['rating'] - 1).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad the paths to be same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find the maximum length of the paths\n",
    "# max_length = max(len(path) for path in paths)\n",
    "\n",
    "# # Pad the paths with zeros\n",
    "# padded_paths = np.zeros((len(paths), max_length), dtype=int)\n",
    "# paths_lengths = []\n",
    "\n",
    "# for i, path in enumerate(paths):\n",
    "#     paths_lengths.append(len(path))\n",
    "#     padded_paths[i, :len(path)] = path\n",
    "\n",
    "# paths_lengths = torch.tensor(paths_lengths)\n",
    "\n",
    "# # Convert padded_paths to a torch tensor\n",
    "# padded_paths = torch.tensor(padded_paths, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathDataset(Dataset):\n",
    "    def __init__(self, all_padded_paths, ratings, node_static_embeddings, edge_features):\n",
    "        self.all_padded_paths = all_padded_paths\n",
    "        self.ratings = ratings\n",
    "        self.node_static_embeddings = node_static_embeddings\n",
    "        self.edge_features = edge_features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_padded_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.all_padded_paths[idx]\n",
    "        rating = self.ratings[idx]\n",
    "\n",
    "        # Extract the 1-hop neighborhood subgraph\n",
    "        neighbors, in_path, subgraph, edges = self.get_path_1hop_neighborhood(\n",
    "            path,\n",
    "            self.edge_features\n",
    "        )\n",
    "\n",
    "        # Get node features\n",
    "        node_features = self.node_static_embeddings[neighbors]\n",
    "\n",
    "        # Create edge index tensor\n",
    "        if len(edges) > 0:\n",
    "            edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "        else:\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "        # Create the Data object\n",
    "        node_data = Data(\n",
    "            x=node_features,\n",
    "            edge_index=edge_index,\n",
    "            y=torch.tensor([rating], dtype=torch.float),\n",
    "            in_path=torch.tensor(in_path, dtype=torch.float),\n",
    "            # We convert the subgraph to numpy to avoid issues with PyTorch's Data collation,\n",
    "            # since the subgraphs have shape NxN, instead of NxD. That is they do not have\n",
    "            # fixed feature dimensionality in the columns. The columns vary with the rows.\n",
    "            adj=subgraph.numpy()\n",
    "        )\n",
    "\n",
    "        return node_data\n",
    "\n",
    "    def get_path_1hop_neighborhood(self, path, edge_features, do_shuffle=False):\n",
    "        path_nodes = set(path)\n",
    "\n",
    "        # Collect 1-hop neighbors in a set to avoid duplicates\n",
    "        neighbors = set()\n",
    "        for node in path:\n",
    "            neighbors.update(edge_features[node].nonzero().squeeze(1).tolist())\n",
    "\n",
    "        all_nodes = list(path_nodes | neighbors)\n",
    "\n",
    "        if do_shuffle:\n",
    "            random.shuffle(all_nodes)\n",
    "\n",
    "        # Extract subgraph using tensor indexing\n",
    "        indices = torch.tensor(all_nodes)\n",
    "        subgraph = edge_features.index_select(0, indices).index_select(1, indices)\n",
    "\n",
    "        # Create a binary list indicating if each node is in the path\n",
    "        is_in_path = [1 if node in path_nodes else 0 for node in all_nodes]\n",
    "\n",
    "        # Edge list from non-zero entries in the subgraph\n",
    "        edge_indices = (subgraph != 0).nonzero(as_tuple=False)\n",
    "        edge_list = [(src.item(), tgt.item()) for src, tgt in edge_indices]\n",
    "\n",
    "        return all_nodes, is_in_path, subgraph, edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Samples: 24225\n",
      "Number of Validation Samples: 1425\n",
      "Number of Testing Samples: 2851\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2  # only works with batch size of 1\n",
    "\n",
    "# Define the split ratio (e.g., 80% for training, 20% for testing)\n",
    "train_ratio = 0.85\n",
    "val_ratio = 0.05\n",
    "test_ratio = 0.1\n",
    "\n",
    "number_of_paths = len(paths)\n",
    "\n",
    "# Calculate the sizes of the splits\n",
    "train_size = int(train_ratio * number_of_paths)\n",
    "val_size = int(val_ratio * number_of_paths)\n",
    "test_size = number_of_paths - train_size - val_size\n",
    "\n",
    "print(\"Number of Training Samples:\", train_size)\n",
    "print(\"Number of Validation Samples:\", val_size)\n",
    "print(\"Number of Testing Samples:\", test_size)\n",
    "\n",
    "full_paths_dataset = PathDataset(\n",
    "    paths,\n",
    "    ratings,\n",
    "    node_static_embeddings,\n",
    "    edge_features,\n",
    ")\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset, val_dataset = random_split(\n",
    "    dataset=full_paths_dataset,\n",
    "    lengths=[train_size, test_size, val_size]\n",
    ")\n",
    "\n",
    "# Create data loaders (optional, but recommended for efficient training)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGELayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(GraphSAGELayer, self).__init__()\n",
    "        self.out_feats = out_feats\n",
    "        self.linear_self = nn.Linear(in_feats, out_feats)\n",
    "        self.linear_neigh = nn.Linear(in_feats, out_feats)\n",
    "        self.output_linear = nn.Linear(2 * out_feats, out_feats)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x: Node features [num_nodes, in_feats]\n",
    "        # edge_index: Graph connectivity [2, num_edges]\n",
    "        # edge_attr: Edge features [num_edges, edge_attr_dim], optional\n",
    "\n",
    "        num_nodes = x.size(0)\n",
    "\n",
    "        # Compute self node embeddings\n",
    "        h_self = F.relu(self.linear_self(x))  # [num_nodes, out_feats]\n",
    "\n",
    "        # Neighbor node indices and features\n",
    "        src_nodes = edge_index[0]  # Source nodes (neighbors)\n",
    "        dst_nodes = edge_index[1]  # Target nodes\n",
    "\n",
    "        # Neighbor features\n",
    "        x_neigh = x[src_nodes]  # [num_edges, in_feats]\n",
    "\n",
    "        # Linear transformation on neighbor features\n",
    "        h_neigh = F.relu(self.linear_neigh(x_neigh))  # [num_edges, out_feats]\n",
    "\n",
    "        # Aggregate neighbor messages using max pooling\n",
    "        h_neigh_agg = torch_scatter.scatter_max(\n",
    "            src=h_neigh.detach().cpu(),\n",
    "            index=dst_nodes,\n",
    "            dim=0,\n",
    "            dim_size=num_nodes\n",
    "        )[0]\n",
    "\n",
    "        # Handle nodes with no incoming edges\n",
    "        h_neigh_agg[h_neigh_agg == float('-inf')] = 0\n",
    "\n",
    "        # Concatenate self and neighbor embeddings\n",
    "        # [num_nodes, 2 * out_feats]\n",
    "        h_concat = torch.cat([h_self, h_neigh_agg], dim=1)\n",
    "\n",
    "        # Output transformation\n",
    "        h_out = F.relu(self.output_linear(h_concat))  # [num_nodes, out_feats]\n",
    "\n",
    "        return h_out\n",
    "\n",
    "\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, out_feats):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.layer1 = GraphSAGELayer(in_feats, hidden_feats)\n",
    "        self.layer2 = GraphSAGELayer(hidden_feats, hidden_feats)\n",
    "        self.layer3 = GraphSAGELayer(hidden_feats, out_feats)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h1 = self.layer1(x, edge_index)\n",
    "        h2 = self.layer2(h1, edge_index)\n",
    "        h3 = self.layer3(h2, edge_index)\n",
    "\n",
    "        return h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, node_in_feats, k_edge_features, hidden_feats, out_feats, class_feats, num_classes, dropout=0.5, bias=True):\n",
    "        super(Model, self).__init__()\n",
    "        self.k_edge_features = k_edge_features\n",
    "\n",
    "        self.node_features_embeds = GraphSAGE(node_in_feats + 1, hidden_feats, out_feats)\n",
    "        self.edge_eigen_mapping = nn.Linear(k_edge_features, node_in_feats)\n",
    "        self.edge_features_embeds = GraphSAGE(node_in_feats + 1, hidden_feats, out_feats)\n",
    "\n",
    "        self.classifier_head = nn.Sequential(\n",
    "            # Concatenated node and edge embeddings\n",
    "            nn.Linear(out_feats * 2, class_feats, bias=bias),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(class_feats, class_feats, bias=bias),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(class_feats, num_classes, bias=bias),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, node_features, edge_subgraph, edge_list, is_in_path, batch_mask):\n",
    "        is_in_path = torch.tensor(is_in_path).unsqueeze(1)\n",
    "\n",
    "        # The edge features (as eigenvectors)\n",
    "        # Remember that edge_subgraph is a list of subgraphs as numpy arrays. We need to convert them to tensors\n",
    "        # and then compute the eigenvectors for each subgraph before concatenating them.\n",
    "        collected_edge_features = []\n",
    "        for subgraph in edge_subgraph:\n",
    "            edge_features = self.get_k_eigenvecs(torch.from_numpy(subgraph), self.k_edge_features)\n",
    "            edge_features = self.edge_eigen_mapping(edge_features.to(torch.float32))\n",
    "            collected_edge_features.append(edge_features)\n",
    "\n",
    "        # Concatenate the edge features\n",
    "        collected_edge_features = torch.cat(collected_edge_features, dim=0)\n",
    "        collected_edge_features = torch.cat((is_in_path, collected_edge_features), dim=1)\n",
    "\n",
    "        # The node features\n",
    "        node_features = torch.cat((is_in_path, node_features), dim=1)\n",
    "\n",
    "        # The node and edge GraphSAGE embeddings\n",
    "        edge_embeds = self.node_features_embeds(collected_edge_features, edge_list)\n",
    "        node_embeds = self.node_features_embeds(node_features, edge_list)\n",
    "\n",
    "        # Combine the node and edge embeddings\n",
    "        h = torch.cat((node_embeds, edge_embeds), dim=1)\n",
    "\n",
    "        # The classifier head\n",
    "        out = self.classifier_head(h)\n",
    "        out = global_mean_pool(out, batch_mask)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def get_k_eigenvecs(self, edge_features, k):\n",
    "        # Compute eigenvalues and eigenvectors of L\n",
    "        eigenvalues, eigenvectors = torch.linalg.eigh(edge_features)  # eigenvalues in ascending order\n",
    "\n",
    "        # Identify non-zero (non-trivial) eigenvalues\n",
    "        eps = 1e-5  # Tolerance for zero\n",
    "        non_zero_indices = (eigenvalues > eps).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        # Handle the case when there are fewer than k non-zero eigenvalues\n",
    "        if len(non_zero_indices) < k:\n",
    "            raise ValueError(f\"Not enough non-trivial eigenvalues, got {len(non_zero_indices)}, need {k}\")\n",
    "\n",
    "        # Get the indices of the k smallest non-zero eigenvalues\n",
    "        k_smallest_indices = non_zero_indices[:k]\n",
    "\n",
    "        # Get the corresponding eigenvectors\n",
    "        k_smallest_eigenvectors = eigenvectors[:, k_smallest_indices]\n",
    "\n",
    "        return k_smallest_eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = Model(\n",
    "    node_static_embeddings.shape[1],\n",
    "    k_edge_features=5,  # Number of eigenvectors to use\n",
    "    hidden_feats=1024,  # Hidden layer size for GraphSAGE\n",
    "    out_feats=768,  # Output layer size for GraphSAGE\n",
    "    class_feats=256,  # Hidden layer size for the classifier head\n",
    "    num_classes=5,  # Number of classes\n",
    "    dropout=0.2,  # Dropout probability\n",
    "    bias=True  # Use bias in the linear layers of the classifier head\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(L.LightningModule):\n",
    "    def __init__(self, model, config):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        logits = self.do_forward(batch)\n",
    "        loss = self.compute_loss(logits, batch.y)\n",
    "\n",
    "        loss = self.compute_loss(logits, batch.y)\n",
    "        acc = self.compute_accuracy(logits, batch.y)\n",
    "\n",
    "        # Log training loss and accuracy\n",
    "        self.log(\"training_loss\", loss, on_step=True, on_epoch=True, logger=True, batch_size=batch_size)\n",
    "        self.log(\"training_accuracy\", acc, on_step=True, on_epoch=True, logger=True, batch_size=batch_size)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        logits = self.do_forward(batch)\n",
    "\n",
    "        loss = self.compute_loss(logits, batch.y)\n",
    "        acc = self.compute_accuracy(logits, batch.y)\n",
    "\n",
    "        self.log(\"test_loss\", loss, on_step=True, on_epoch=True, batch_size=batch_size)\n",
    "        self.log(\"test_accuracy\", acc, on_step=True, on_epoch=True, batch_size=batch_size)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logits = self.do_forward(batch)\n",
    "\n",
    "        loss = self.compute_loss(logits, batch.y)\n",
    "        acc = self.compute_accuracy(logits, batch.y)\n",
    "\n",
    "        # Log validation loss and accuracy\n",
    "        self.log(\"validation_loss\", loss, on_step=True, on_epoch=True, logger=True, batch_size=batch_size)\n",
    "        self.log(\"validation_accuracy\", acc, on_step=True, on_epoch=True, logger=True, batch_size=batch_size)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            params=self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            betas=(self.config.beta1, self.config.beta2),\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": ReduceLROnPlateau(optimizer),\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "                \"monitor\": \"validation_loss\",\n",
    "                \"strict\": True,\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def do_forward(self, batch):\n",
    "        \"\"\"This is a custom method not associated with Lightning\"\"\"\n",
    "        return self.model(batch.x, batch.adj, batch.edge_index, batch.in_path, batch.batch)\n",
    "\n",
    "    def compute_loss(self, logits, targets):\n",
    "        \"\"\"This is a custom method not associated with Lightning\"\"\"\n",
    "        targets = targets.type(torch.long)\n",
    "        return F.cross_entropy(logits, targets)\n",
    "\n",
    "    def compute_accuracy(self, logits, targets):\n",
    "        \"\"\"This is a custom method not associated with Lightning\"\"\"\n",
    "        return (logits.argmax(1) == targets).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    learning_rate: float = 1e-4\n",
    "    weight_decay: float = 1e-2\n",
    "    beta1: float = 0.9\n",
    "    beta2: float = 0.999\n",
    "    grad_clip: float = 1.0\n",
    "    num_epochs: int = 10\n",
    "    log_interval: int = 10\n",
    "    grad_accumulation: int = 16\n",
    "    wandb_project: str = \"wikispeedia\"\n",
    "\n",
    "\n",
    "config = ModelConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/faustogerman/miniforge3/envs/geometric/lib/python3.9/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfaustotnc\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20241028_002301-yu31a0sy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/faustotnc/wikispeedia/runs/yu31a0sy' target=\"_blank\">Oct-28-24 @ 12:23 AM</a></strong> to <a href='https://wandb.ai/faustotnc/wikispeedia' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/faustotnc/wikispeedia' target=\"_blank\">https://wandb.ai/faustotnc/wikispeedia</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/faustotnc/wikispeedia/runs/yu31a0sy' target=\"_blank\">https://wandb.ai/faustotnc/wikispeedia/runs/yu31a0sy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type  | Params | Mode \n",
      "----------------------------------------\n",
      "0 | model | Model | 24.9 M | train\n",
      "----------------------------------------\n",
      "24.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.9 M    Total params\n",
      "99.481    Total estimated model params size (MB)\n",
      "38        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faustogerman/miniforge3/envs/geometric/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/var/folders/ww/02m6b2_s1bl0sbzz4jq19n500000gn/T/ipykernel_23241/3675986996.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  is_in_path = torch.tensor(is_in_path).unsqueeze(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faustogerman/miniforge3/envs/geometric/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 66/12113 [00:38<1:57:45,  1.70it/s, v_num=a0sy]"
     ]
    }
   ],
   "source": [
    "# model\n",
    "transformer = LitModel(MODEL, config)\n",
    "\n",
    "# logging\n",
    "wandb_logger = WandbLogger(\n",
    "    project=config.wandb_project,\n",
    "    name=datetime.datetime.now().strftime(\"%b-%d-%y @ %I:%M %p\"),\n",
    "    log_model=\"all\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "wandb_logger\n",
    "\n",
    "# Define the trainer\n",
    "trainer = L.Trainer(\n",
    "    default_root_dir=\"some/path/\",\n",
    "    max_epochs=config.num_epochs,\n",
    "    val_check_interval=0.5,\n",
    "    log_every_n_steps=config.log_interval,\n",
    "    accumulate_grad_batches=config.grad_accumulation,\n",
    "    gradient_clip_val=config.grad_clip,\n",
    "    profiler=\"simple\",\n",
    "    logger=wandb_logger,\n",
    "    # NOTE: Should be commented out if using GPU. There may be bugs related\n",
    "    # to tensor device placements, since I have not tested this on GPU.\n",
    "    accelerator=\"cpu\",\n",
    ")\n",
    "\n",
    "# train model\n",
    "trainer.fit(\n",
    "    model=transformer,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "trainer.test(model=transformer, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
